{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33987e86-4e26-49dd-92e2-f26260083b61",
   "metadata": {},
   "source": [
    "# Light Gradient Boosting Machine (LightGBM)\n",
    "\n",
    "### LightGBM (Light Gradient Boosting Machine), açık kaynaklı bir makine öğrenimi algoritmasıdır. XGBoost algoritmasının bir geliştirmesi olarak geliştirilmiştir ve özellikle büyük ölçekli veri setleri üzerinde yüksek performans sunmak amacıyla tasarlanmıştır. LightGBM, düşük bellek tüketimi, hızlı eğitim süreleri ve yüksek tahmin doğruluğu gibi avantajlara sahiptir.\n",
    "\n",
    "### LightGBM, bir gradient boosting framework'ü olarak çalışır. Gradient Boosting, zayıf öğrenicilerin (genellikle karar ağaçları) bir araya getirilerek güçlü bir modelin oluşturulmasını sağlayan bir ensemble öğrenme tekniğidir. Gradient boosting, önceki öğrenicinin hatalarına odaklanır ve ardışık olarak yeni öğrenicileri bu hataları düzeltmek için eğitir. Bu şekilde, her bir öğrenici bir öncekine göre daha iyi hataları düzeltebilir ve nihai tahmin doğruluğu artırılır.\n",
    "\n",
    "## LightGBM, diğer geleneksel gradient boosting algoritmalarından farklı olarak bazı özelliklere sahiptir: \n",
    "\n",
    "### Düşük Bellek Tüketimi: LightGBM, büyük veri setlerinde bile düşük bellek kullanımı sağlar. Veri setini küçük parçalara böler ve paralel olarak eğitim yapar.\n",
    "\n",
    "### Hızlı Eğitim Süreleri: LightGBM, özellikle büyük veri setlerinde hızlı eğitim süreleri sunar. Bu hızlı eğitim, özellikle hiperparametre optimizasyonu ve model tuning aşamalarında verimliliği artırır.\n",
    "\n",
    "### Yüksek Tahmin Doğruluğu: LightGBM, düşük bellek tüketimi ve hızlı eğitim süreleriyle birlikte yüksek tahmin doğruluğu sağlar. Bu özellik, özellikle sınıflandırma ve regresyon problemlerinde başarılı sonuçlar elde etmek için değerlidir.\n",
    "\n",
    "## LightGBM ayrıca çeşitli önemli özellikler sunar: \n",
    "\n",
    "### Öznitelik İmportansı: LightGBM, eğitim sonrası her bir öznitelik için öenm puanlarını hesaplayabilir. Bu, hangi özniteliklerin modelin tahminlerinde daha büyük bir etkiye sahip olduğunu belirlemeye yardımcı olur.\n",
    "\n",
    "### Kategorik Öznitelik Desteği: LightGBM, kategorik öznitelikleri doğrudan kullanabilir ve bu öznitelikleri dönüştürme gereği duymaz. \n",
    "\n",
    "### Early Stopping: LightGBM, eğitim sırasında erken durdurma (early stopping) mekanizması sunar. Bu, eğitim sürecini otomatik olarak sonlandırır ve overfitting'i önlemeye yardımcı olur.\n",
    "\n",
    "### Paralel ve Dağıtık Eğitim: LightGBM, paralel ve dağıtık eğitimi destekler, bu da birden fazla işlemci ve çoklu makine ortamlarında eğitimi hızlandırır.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd33ca01-7ffb-457b-b214-151069be673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033a29c0-fa95-4652-84f7-d855bc8f2070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'silent': 'warn',\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LGBMRegressor()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585ea00-8317-4355-a675-21119cd9c973",
   "metadata": {},
   "source": [
    "### boosting_type (string, default='gbdt'): LightGBM'in kullanacağı boosting tipini belirtir. Çeşitli seçenekler arasında 'gbdt' (Gradient Boosting Decision Tree), 'dart' (Dropouts Meet Multiple Additive Regression Trees) ve 'goss' (Gradient-based One-Side Sampling) bulunur.\n",
    "\n",
    "### num_leaves (int, default=31): Bir karar ağacının yaprak düğümlerinin maksimum sayısını belirtir. Bu sayı arttıkça modelin karmaşıklığı ve kapasitesi artar. (2, 131072) değer aralığında değer alabilir.\n",
    "\n",
    "### learning_rate (float, default=0.1): Öğrenme oranıdır. Her bir ağaç eğitimi aşamasında kullanılan öğrenme oranıdır. Düşük bir değer, daha fazla ağaç ve daha yavaş öğrenmeyi ifade eder. (0.001, 1.0) değer aralığında değer alabilir.\n",
    "\n",
    "### n_estimators (int, default=100): Oluşturulacak toplam ağaç sayısını belirtir. Daha yüksek bir değer, daha fazla ağaç ve daha yüksek bir model karmaşıklığına yol açar. (20, 1000) değer aralığında değer alabilir.\n",
    "\n",
    "### subsample_for_bin (int, default=200000): Sayısal özellikleri bir aralığa bölmek için kullanılan örnekleme sayısını belirtir. Büyük bir değer, daha iyi bir özellik bölmesi için daha fazla örnekleme anlamına gelir, ancak daha büyük hesaplama maliyetiyle birlikte gelir. (20000, 500000) değer aralığında değer alabilir.\n",
    "\n",
    "### objective (string or calleble, default=None): Kayıp fonksiyonunu belirtir. Örnek olarak 'regression', 'binary' veya 'multiclass' gibi değerler alabilir. Ayrıca, özel bir kayıp fonksiyonu da belirtilebilir.\n",
    "\n",
    "### min_child_samples (int, default=20): Bir yaprak düğümü oluşturmak için gereken minimum örnek sayısını belirtir. Daha küçük bir değer, modelin daha fazla detayı yakalamasını sağlar, ancak aşırı uyuma yol açabilir. (1, 200) değer aralığında değer alabilir.\n",
    "\n",
    "### reg_alpha (float, default=0.0): L1 düzenlileştirmesinin gücünü belirleyen bir parametredir. Daha büyük bir değer, daha fazla düzenlileştirme yapar. (0.0, 1.0) değer aralığında değer alabilir.\n",
    "\n",
    "### reg_lambda (float, default=0.0): L2 düzenlileştirmesinin gücünü belirleyen bir parametredir. Daha büyük bir değer, daha fazla düzenlileştirme yapar. (0.0, 1.0) değer aralığında değer alabilir.\n",
    "\n",
    "### colsample_bytree (float, default=1.0): Her ağaç eğitimi sırasında kullanılacak özelliklerin yüzdesini belirtir. Daha küçük bir değer, daha fazla özellik rastgele seçilir ve modelin daha genelleştirici olmasını sağlar. (0.1, 1.0) değer aralığında değer alabilir.\n",
    "\n",
    "\n",
    "### Bu, bazı temel hiperparametrelerin yanı sıra daha birçok hiperparametrenin olduğu anlamına gelir. Bu hiperparametrelerin değerleri, veri setine ve soruna bağlı olarak değişebilir. Genellikle, hiperparametrelerin belirlenmesi deneme yanılma yöntemiyle veya hiperparametre optimizasyonu algoritmalarıyla yapılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf6f479-3a16-497f-90a2-a097adc71e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m           LGBMRegressor\n",
       "\u001b[1;31mString form:\u001b[0m    LGBMRegressor()\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\murat\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\n",
       "\u001b[1;31mDocstring:\u001b[0m      LightGBM regressor.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'goss', Gradient-based One-Side Sampling.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int, optional (default=-1)\n",
       "    Number of parallel threads.\n",
       "silent : bool, optional (default=True)\n",
       "    Whether to print messages while running boosting.\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess`` or\n",
       "``objective(y_true, y_pred, group) -> grad, hess``:\n",
       "\n",
       "    y_true : array-like of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    group : array-like\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
       "If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
       "and you should group grad and hess in this way as well.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
